{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all modules needed for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# The below suppresses all warnings in the notebook\n",
    "# Only leave this uncommented for display purposes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below creates a binary classification dataset by creating 3D gaussians and drawing sample points from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and covariance for Class 0\n",
    "mean0 = [0, 0, 0]\n",
    "cov0 = [[2550, 2000, 1500], [2000, 1500, 1200], [1500, 1200, 1900]]  \n",
    "\n",
    "# Number of datapoints for class 0\n",
    "m0 = 100\n",
    "\n",
    "x0_1, x0_2, x0_3 = np.random.multivariate_normal(mean0, cov0, m0).T\n",
    "\n",
    "# Concatenate the 3 dimensions of each feature to create the data matrix for class 0 \n",
    "X0 = np.concatenate((x0_1.reshape(-1, 1), x0_2.reshape(-1, 1), x0_3.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# Create the target vector for class 0 (target is coded with zero)\n",
    "X0_target = np.zeros((m0,), dtype=np.int).reshape(-1, 1)\n",
    "\n",
    "# Mean and covariance for Class 1\n",
    "mean1 = [3, 3, 3]\n",
    "cov1 = [[2550, 2000, 1500], [2000, 1500, 1200], [1500, 1200, 1900]] \n",
    "\n",
    "# Number of datapoints for class 1\n",
    "m1 = 100\n",
    "\n",
    "# Generate class 1 data points from a multivariate (3D) Gaussian distribution\n",
    "#    Here x1_1, x1_2 and x1_3 are 2 dimensions for each data (feature) point\n",
    "x1_1, x1_2, x1_3 = np.random.multivariate_normal(mean1, cov1, m1).T\n",
    "\n",
    "# Concatenate the 3 dimensions of each feature to create the data matrix for class 1\n",
    "X1 = np.concatenate((x1_1.reshape(-1, 1), x1_2.reshape(-1, 1), x1_3.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# Create the target vector for class 1 (target is coded with one)\n",
    "X1_target = np.ones((m1,), dtype=np.int).reshape(-1, 1)\n",
    "\n",
    "#  Class 0 and 1 data are combined to create a single data matrix X\n",
    "X = np.append(X0, X1, axis=0)\n",
    "\n",
    "# Target values for class 0 & 1 are combined to create a single target vector\n",
    "y = np.concatenate((X0_target, X1_target), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the above-created distributions the mean is the same for all.<br>\n",
    "In addition the covariance is not too different either.<br>\n",
    "What this will do is provide a situation in which our approach of KNN fails badly, as will be seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first partition our data into testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 3)\n",
      "(40, 3)\n",
      "(160, 1)\n",
      "(40, 1)\n"
     ]
    }
   ],
   "source": [
    "test_frac = 0.2\n",
    "\n",
    "# Create training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_frac, random_state=0)\n",
    "# Note y correspond to the target vector\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below box standardizes our feature vectors.<br>\n",
    "To test the impact of not standardizing these feature vectors this box could be left uncommented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scale(X_train)\n",
    "X_test = scale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform cross-validation to determine the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.561382\n",
      "Optimal Hyperparameter Values:  {'n_neighbors': 5, 'p': 5, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(1,37, 2), 'p': [1, 2, 5,  10, 20, 30, 50, 100], 'weights': [\"uniform\", \"distance\"]}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn_cv = GridSearchCV(knn, param_grid, scoring='f1', cv=3)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: %f\" % knn_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", knn_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now keep the optimal hyperparameters values, as indicated above, and use these to fit our model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=7, p=20,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_neighbors = 7\n",
    "optimal_p = 20\n",
    "optimal_weight = \"uniform\"\n",
    "\n",
    "knn = KNeighborsClassifier(weights=optimal_weight, algorithm='auto', n_neighbors=optimal_neighbors, p=optimal_p)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate our training data using the below error metrics. For this we chose to keep the default decision threshold (0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The below are the performance measures using the testing dataset:\n",
      "\n",
      "Accuracy (Threshold 0.50) = 0.550000\n",
      "Precision (Threshold 0.50) = 0.583333\n",
      "Recall (Threshold 0.50) = 0.636364\n",
      "F1 Score = (Threshold 0.50) = 0.608696\n"
     ]
    }
   ],
   "source": [
    "print('The below are the performance measures using the testing dataset:')\n",
    "\n",
    "thresh = 0.5 # Chosen optimal threshold \n",
    "\n",
    "y_test_predicted = (knn.predict_proba(X_test)[:,1] >= thresh).astype(bool)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_predicted) \n",
    "print(\"\\nAccuracy (Threshold %.2f) = %f\" % (thresh, accuracy))\n",
    "\n",
    "precision = precision_score(y_test, y_test_predicted) \n",
    "print(\"Precision (Threshold %.2f) = %f\" % (thresh, precision))\n",
    "\n",
    "recall = recall_score(y_test, y_test_predicted)\n",
    "print(\"Recall (Threshold %.2f) = %f\" % (thresh, recall))\n",
    "\n",
    "f1 = f1_score(y_test, y_test_predicted)\n",
    "print(\"F1 Score = (Threshold %.2f) = %f\" % (thresh, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these error metrics indicate that our model has failed.<br>\n",
    "This was expected as KNN requires us to find a clear decision boundary, which does not work well if the classes are too interrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see how well these parameters work by using the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The below are the performance measures using the testing dataset:\n",
      "\n",
      "Accuracy (Threshold 0.50) = 0.450000\n",
      "Precision (Threshold 0.50) = 0.000000\n",
      "Recall (Threshold 0.50) = 0.000000\n",
      "F1 Score = (Threshold 0.50) = 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Use the Mahalanobis distance metric\n",
    "knn = 15; # Number of nearest neighbors to use\n",
    "weight_fun = \"distance\" # Weight type to use\n",
    "\n",
    "# Calculate the Covariance Matrix for the training features\n",
    "covarianceMatrix = np.cov(X_train)\n",
    "# Calculate the inverse of the Covariance Matrix for the training features\n",
    "\n",
    "invCovarianceMatrix = np.linalg.inv(covarianceMatrix) # Use if it does not give an error\n",
    "# The below will employ a pseudo-inverse in the case that it is not possible to calculate the exact inverse\n",
    "# invCovarianceMatrix = np.linalg.pinv(covarianceMatrix) # Use if above gives an error\n",
    "\n",
    "# Using best hyperparameter values found above we find the optimum model\n",
    "knn = KNeighborsClassifier(weights=weight_fun, algorithm='brute',\n",
    "n_neighbors=knn, metric = \"mahalanobis\", metric_params={'V':\n",
    "covarianceMatrix, 'VI':invCovarianceMatrix})\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print('The below are the performance measures using the testing dataset:')\n",
    "\n",
    "thresh = 0.5 # Chosen optimal threshold \n",
    "\n",
    "y_test_predicted = (knn.predict_proba(X_test)[:,1] >= thresh).astype(bool)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_predicted) \n",
    "print(\"\\nAccuracy (Threshold %.2f) = %f\" % (thresh, accuracy))\n",
    "\n",
    "precision = precision_score(y_test, y_test_predicted) \n",
    "print(\"Precision (Threshold %.2f) = %f\" % (thresh, precision))\n",
    "\n",
    "recall = recall_score(y_test, y_test_predicted)\n",
    "print(\"Recall (Threshold %.2f) = %f\" % (thresh, recall))\n",
    "\n",
    "f1 = f1_score(y_test, y_test_predicted)\n",
    "print(\"F1 Score = (Threshold %.2f) = %f\" % (thresh, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
